{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "cell_execution_strategy": "setup"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNl102mrRggs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/training_data.csv')"
      ],
      "metadata": {
        "id": "ngVscLXDUYkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the specified columns\n",
        "columns_to_drop = ['id', 'Date', 'review ID', 'reviewer ID', 'product ID']\n",
        "df = df.drop(columns=columns_to_drop)\n"
      ],
      "metadata": {
        "id": "pnztJhc6U2ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LE = LabelEncoder()\n",
        "df['Label_encoded'] = LE.fit_transform(df['Label'])"
      ],
      "metadata": {
        "id": "R_neOAElZvq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = pd.DataFrame(df['Label'].value_counts())\n",
        "label_counts\n"
      ],
      "metadata": {
        "id": "GsacSdqwae1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_values = list(label_counts.index)\n",
        "order = list(pd.DataFrame(df['Label_encoded'].value_counts()).index)\n",
        "label_values = [l for _,l in sorted(zip(order, label_values))]\n",
        "\n",
        "label_values"
      ],
      "metadata": {
        "id": "wfjvvPd1bIE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['reviews'].values\n",
        "labels = df['Label_encoded'].values\n"
      ],
      "metadata": {
        "id": "pQD9qCfvbV2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_lengths = [len(texts[i].split()) for i in range(len(texts))]\n",
        "print(min(text_lengths))\n",
        "print(max(text_lengths))"
      ],
      "metadata": {
        "id": "bPFiw-jmbb13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)\n",
        "\n",
        "print('Original Text: ', texts[0], '\\n')\n",
        "print('Tokenized Text: ', tokenizer.tokenize(texts[0]), '\\n')\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(texts[0])))\n",
        "\n"
      ],
      "metadata": {
        "id": "iFPdSVwEbeKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_ids = [tokenizer.encode(text, max_length=300, pad_to_max_length=True) for text in texts]\n",
        "\n",
        "text_ids[0]"
      ],
      "metadata": {
        "id": "PKPdOR8WbhfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_ids_lengths = [len(text_ids[i]) for i in range(len(text_ids))]\n",
        "print(min(text_ids_lengths))\n",
        "print(max(text_ids_lengths))"
      ],
      "metadata": {
        "id": "8kZaUC_xbtSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "att_masks = []\n",
        "for ids in text_ids:\n",
        "    masks = [int(id > 0) for id in ids]\n",
        "    att_masks.append(masks)\n",
        "\n",
        "att_masks[0]"
      ],
      "metadata": {
        "id": "HutEQe3gdTXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_x, test_val_x, train_y, test_val_y = train_test_split(text_ids, labels, random_state=111, test_size=0.2)\n",
        "train_m, test_val_m = train_test_split(att_masks, random_state=111, test_size=0.2)\n",
        "\n",
        "test_x, val_x, test_y, val_y = train_test_split(test_val_x, test_val_y, random_state=111, test_size=0.5)\n",
        "test_m, val_m = train_test_split(test_val_m, random_state=111, test_size=0.5)"
      ],
      "metadata": {
        "id": "-2_inNSYdVVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "train_x = torch.tensor(train_x)\n",
        "test_x = torch.tensor(test_x)\n",
        "val_x = torch.tensor(val_x)\n",
        "train_y = torch.tensor(train_y)\n",
        "test_y = torch.tensor(test_y)\n",
        "val_y = torch.tensor(val_y)\n",
        "train_m = torch.tensor(train_m)\n",
        "test_m = torch.tensor(test_m)\n",
        "val_m = torch.tensor(val_m)\n",
        "\n",
        "print(train_x.shape)\n",
        "print(test_x.shape)\n",
        "print(val_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_y.shape)\n",
        "print(val_y.shape)\n",
        "print(train_m.shape)\n",
        "print(test_m.shape)\n",
        "print(val_m.shape)"
      ],
      "metadata": {
        "id": "Jr8QWFcJdYyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_x, train_m, train_y)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "val_data = TensorDataset(val_x, val_m, val_y)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "9pKrlRqudbZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertForSequenceClassification, AdamW, DistilBertConfig\n",
        "\n",
        "num_labels = len(set(labels))\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels,\n",
        "                                                            output_attentions=False, output_hidden_states=False)"
      ],
      "metadata": {
        "id": "nFQr27FldfzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "model = model.to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "p6DKR8ZidpJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print('Number of trainable parameters:', count_parameters(model), '\\n', model)"
      ],
      "metadata": {
        "id": "PdrFEYsIdyqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[n for n, p in model.named_parameters()]"
      ],
      "metadata": {
        "id": "ALpLNSf1dzSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-5\n",
        "adam_epsilon = 1e-8\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.2},\n",
        "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)"
      ],
      "metadata": {
        "id": "tBFN4jQFd3kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "num_epochs = 10\n",
        "total_steps = len(train_dataloader) * num_epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
      ],
      "metadata": {
        "id": "STRI_O-Zd6on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n"
      ],
      "metadata": {
        "id": "cWyhlADeeF0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "seed_val = 111\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "wAWoez1xeJws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "num_mb_train = len(train_dataloader)\n",
        "num_mb_val = len(val_dataloader)\n",
        "\n",
        "if num_mb_val == 0:\n",
        "    num_mb_val = 1\n",
        "\n",
        "for n in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    val_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for k, (mb_x, mb_m, mb_y) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        model.train()\n",
        "\n",
        "        mb_x = mb_x.to(device)\n",
        "        mb_m = mb_m.to(device)\n",
        "        mb_y = mb_y.to(device)\n",
        "\n",
        "        outputs = model(mb_x, attention_mask=mb_m, labels=mb_y)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        #loss = model_loss(outputs[1], mb_y)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        train_loss += loss.data / num_mb_train\n",
        "\n",
        "    print (\"\\nTrain loss after itaration %i: %f\" % (n+1, train_loss))\n",
        "    train_losses.append(train_loss.cpu())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "        for k, (mb_x, mb_m, mb_y) in enumerate(val_dataloader):\n",
        "            mb_x = mb_x.to(device)\n",
        "            mb_m = mb_m.to(device)\n",
        "            mb_y = mb_y.to(device)\n",
        "\n",
        "            outputs = model(mb_x, attention_mask=mb_m, labels=mb_y)\n",
        "\n",
        "            loss = outputs[0]\n",
        "            #loss = model_loss(outputs[1], mb_y)\n",
        "\n",
        "            val_loss += loss.data / num_mb_val\n",
        "\n",
        "        print (\"Validation loss after itaration %i: %f\" % (n+1, val_loss))\n",
        "        val_losses.append(val_loss.cpu())\n",
        "\n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    print(f'Time: {epoch_mins}m {epoch_secs}s')"
      ],
      "metadata": {
        "id": "J-RxSfxVeLq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "out_dir = './model'\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "model_to_save.save_pretrained(out_dir)\n",
        "tokenizer.save_pretrained(out_dir)\n",
        "\n",
        "with open(out_dir + '/train_losses.pkl', 'wb') as f:\n",
        "    pickle.dump(train_losses, f)\n",
        "\n",
        "with open(out_dir + '/val_losses.pkl', 'wb') as f:\n",
        "    pickle.dump(val_losses, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "DCP-J56utVPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_dir = './model'\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained(out_dir)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "\n",
        "with open(out_dir + '/train_losses.pkl', 'rb') as f:\n",
        "    train_losses = pickle.load(f)\n",
        "\n",
        "with open(out_dir + '/val_losses.pkl', 'rb') as f:\n",
        "    val_losses = pickle.load(f)"
      ],
      "metadata": {
        "id": "OJdoOgfktYNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_losses)\n"
      ],
      "metadata": {
        "id": "RZTH1TsctbUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(val_losses)\n",
        "\n"
      ],
      "metadata": {
        "id": "pgO-x7kwtdXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "test_data = TensorDataset(test_x, test_m)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "outputs = []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for k, (mb_x, mb_m) in enumerate(test_dataloader):\n",
        "        mb_x = mb_x.to(device)\n",
        "        mb_m = mb_m.to(device)\n",
        "        output = model(mb_x, attention_mask=mb_m)\n",
        "        outputs.append(output[0].to('cpu'))\n",
        "\n",
        "outputs = torch.cat(outputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "3x93Sy7btfsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, predicted_values = torch.max(outputs, 1)\n",
        "predicted_values = predicted_values.numpy()\n",
        "true_values = test_y.numpy()"
      ],
      "metadata": {
        "id": "q_g7E6ibtyKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "test_accuracy = np.sum(predicted_values == true_values) / len(true_values)\n",
        "print (\"Test Accuracy:\", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "id": "cMVFjTkot1Cj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}